{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow - TensorRT Inference from Checkpoint with Tensorflow 1.14, 1.15\n",
    "In this notebook, we demonstrate the process to create a TF-TRT optimized model  for inference from a Tensorflow *checkpoint*. We will also validate the resulting models, both on accuracy and speed.\n",
    "\n",
    "This notebook was designed to run with TensorFlow versions 1.14, 1.15 which is included as part of NVIDIA NGC Tensorflow containers from `nvcr.io/nvidia/tensorflow:19.07-py3` to `nvcr.io/nvidia/tensorflow:19.12-tf1-py3` that can be downloaded from http://ngc.nvidia.com.\n",
    "\n",
    "## Notebook  Content\n",
    "1. [Pre-requisite: data and model](#1)\n",
    "1. [Verifying the orignal FP32 model](#2)\n",
    "1. [Creating TF-TRT FP32 model](#3)\n",
    "1. [Creating TF-TRT FP16 model](#4)\n",
    "1. [Creating TF-TRT INT8 model](#5)\n",
    "1. [Calibrating TF-TRT INT8 model with raw JPEG images](#6)\n",
    "\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "We wil be using the ImageNet dataset in TFrecords format. Google provides an excellent all-in-one script for downloading and preparing the ImageNet dataset at \n",
    "\n",
    "https://github.com/tensorflow/models/blob/master/research/inception/inception/data/download_and_preprocess_imagenet.sh.\n",
    "\n",
    "We will run this demonstration with a saved model from the Tensorflow Slim model zoo \n",
    "\n",
    "https://github.com/tensorflow/models/tree/master/research/slim\n",
    "\n",
    "To run this notebook, start the NGC TF container providing correct path to ImageNet validation data and a TF Slim saved checkpoint:\n",
    "\n",
    "```bash\n",
    "nvidia-docker run -it -p 8888:8888 -v /path/to/image_net/:/data  -v /path/to/saved_model:/saved_model --name TFTRT nvcr.io/nvidia/tensorflow:19.04-py3\n",
    "```\n",
    "Then start Jupyter notebook within the container with:\n",
    "\n",
    "```bash\n",
    "jupyter notebook --ip 0.0.0.0 --port 8888  --allow-root\n",
    "```\n",
    "\n",
    "Connect to Jupyter notebook web interface from your local host http://localhost:8888. \n",
    "\n",
    "<a id=\"1\"></a>\n",
    "## 1. Pre-requisite: data and model\n",
    "\n",
    "We first install some extra packages and external dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/nvidia-examples/tensorrt/tftrt/examples/object_detection /home/vinhngx/vinh-all-data/TME_projects/tensorrt/tftrt/examples/image-classification\n",
      "Setup local variables...\n",
      "Download protobuf...\n",
      "/workspace/nvidia-examples/tensorrt/tftrt/examples/object_detection/protoc /workspace/nvidia-examples/tensorrt/tftrt/examples/object_detection\n",
      "Archive:  protoc-3.7.1-linux-x86_64.zip\n",
      "  inflating: include/google/protobuf/wrappers.proto  \n",
      "  inflating: include/google/protobuf/field_mask.proto  \n",
      "  inflating: include/google/protobuf/api.proto  \n",
      "  inflating: include/google/protobuf/struct.proto  \n",
      "  inflating: include/google/protobuf/descriptor.proto  \n",
      "  inflating: include/google/protobuf/timestamp.proto  \n",
      "  inflating: include/google/protobuf/compiler/plugin.proto  \n",
      "  inflating: include/google/protobuf/empty.proto  \n",
      "  inflating: include/google/protobuf/any.proto  \n",
      "  inflating: include/google/protobuf/source_context.proto  \n",
      "  inflating: include/google/protobuf/type.proto  \n",
      "  inflating: include/google/protobuf/duration.proto  \n",
      "  inflating: bin/protoc              \n",
      "  inflating: readme.txt              \n",
      "/workspace/nvidia-examples/tensorrt/tftrt/examples/object_detection\n",
      "Compile object detection protobuf files...\n",
      "/workspace/nvidia-examples/tensorrt/tftrt/examples/third_party/models/research /workspace/nvidia-examples/tensorrt/tftrt/examples/object_detection\n",
      "/workspace/nvidia-examples/tensorrt/tftrt/examples/object_detection\n",
      "Install tensorflow/models/research...\n",
      "/workspace/nvidia-examples/tensorrt/tftrt/examples/third_party/models/research /workspace/nvidia-examples/tensorrt/tftrt/examples/object_detection\n",
      "Processing /workspace/nvidia-examples/tensorrt/tftrt/examples/third_party/models/research\n",
      "Requirement already satisfied: Pillow>=1.0 in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (6.2.1)\n",
      "Requirement already satisfied: Matplotlib>=2.1 in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (3.1.1)\n",
      "Requirement already satisfied: Cython>=0.28.1 in /usr/local/lib/python3.6/dist-packages (from object-detection==0.1) (0.29.14)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from Matplotlib>=2.1->object-detection==0.1) (2.4.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from Matplotlib>=2.1->object-detection==0.1) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from Matplotlib>=2.1->object-detection==0.1) (1.17.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from Matplotlib>=2.1->object-detection==0.1) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from Matplotlib>=2.1->object-detection==0.1) (2.8.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->Matplotlib>=2.1->object-detection==0.1) (1.13.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->Matplotlib>=2.1->object-detection==0.1) (42.0.2)\n",
      "Building wheels for collected packages: object-detection\n",
      "  Building wheel for object-detection (setup.py): started\n",
      "  Building wheel for object-detection (setup.py): finished with status 'done'\n",
      "  Created wheel for object-detection: filename=object_detection-0.1-cp36-none-any.whl size=987374 sha256=56ba62de2c83d7cedf4af883d1f8179795acdab3db0009029efdcd219744a68f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-n8d6t2el/wheels/93/f7/00/40041de75080d10a353dd56732edd107a88f2bcda8d695b1f9\n",
      "Successfully built object-detection\n",
      "Installing collected packages: object-detection\n",
      "  Found existing installation: object-detection 0.1\n",
      "    Uninstalling object-detection-0.1:\n",
      "      Successfully uninstalled object-detection-0.1\n",
      "Successfully installed object-detection-0.1\n",
      "/workspace/nvidia-examples/tensorrt/tftrt/examples/object_detection\n",
      "Install tensorflow/models/research/slim...\n",
      "/workspace/nvidia-examples/tensorrt/tftrt/examples/third_party/models/research/slim /workspace/nvidia-examples/tensorrt/tftrt/examples/object_detection\n",
      "Processing /workspace/nvidia-examples/tensorrt/tftrt/examples/third_party/models/research/slim\n",
      "Building wheels for collected packages: slim\n",
      "  Building wheel for slim (setup.py): started\n",
      "  Building wheel for slim (setup.py): finished with status 'done'\n",
      "  Created wheel for slim: filename=slim-0.1-cp36-none-any.whl size=215537 sha256=50521e6dc476ec5f953cadc2e65f741918eb9a936f8b335a9c182f0c670a01f7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-sb9cezs8/wheels/81/06/c9/0732786e00652cfe9b779c0266c559f0c85f95e93358d5131e\n",
      "Successfully built slim\n",
      "Installing collected packages: slim\n",
      "  Found existing installation: slim 0.1\n",
      "    Uninstalling slim-0.1:\n",
      "      Successfully uninstalled slim-0.1\n",
      "Successfully installed slim-0.1\n",
      "/workspace/nvidia-examples/tensorrt/tftrt/examples/object_detection\n",
      "Install cocodataset/cocoapi/PythonAPI...\n",
      "/workspace/nvidia-examples/tensorrt/tftrt/examples/third_party/cocoapi/PythonAPI /workspace/nvidia-examples/tensorrt/tftrt/examples/object_detection\n",
      "running build_ext\n",
      "skipping 'pycocotools/_mask.c' Cython extension (up-to-date)\n",
      "copying build/lib.linux-x86_64-3.6/pycocotools/_mask.cpython-36m-x86_64-linux-gnu.so -> pycocotools\n",
      "python setup.py build_ext --inplace\n",
      "running build_ext\n",
      "skipping 'pycocotools/_mask.c' Cython extension (up-to-date)\n",
      "copying build/lib.linux-x86_64-3.6/pycocotools/_mask.cpython-36m-x86_64-linux-gnu.so -> pycocotools\n",
      "rm -rf build\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing pycocotools.egg-info/PKG-INFO\n",
      "writing dependency_links to pycocotools.egg-info/dependency_links.txt\n",
      "writing requirements to pycocotools.egg-info/requires.txt\n",
      "writing top-level names to pycocotools.egg-info/top_level.txt\n",
      "reading manifest file 'pycocotools.egg-info/SOURCES.txt'\n",
      "writing manifest file 'pycocotools.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib.linux-x86_64-3.6\n",
      "creating build/lib.linux-x86_64-3.6/pycocotools\n",
      "copying pycocotools/coco.py -> build/lib.linux-x86_64-3.6/pycocotools\n",
      "copying pycocotools/__init__.py -> build/lib.linux-x86_64-3.6/pycocotools\n",
      "copying pycocotools/mask.py -> build/lib.linux-x86_64-3.6/pycocotools\n",
      "copying pycocotools/cocoeval.py -> build/lib.linux-x86_64-3.6/pycocotools\n",
      "running build_ext\n",
      "skipping 'pycocotools/_mask.c' Cython extension (up-to-date)\n",
      "building 'pycocotools._mask' extension\n",
      "creating build/common\n",
      "creating build/temp.linux-x86_64-3.6\n",
      "creating build/temp.linux-x86_64-3.6/pycocotools\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I../common -I/usr/include/python3.6m -c ../common/maskApi.c -o build/temp.linux-x86_64-3.6/../common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I../common -I/usr/include/python3.6m -c pycocotools/_mask.c -o build/temp.linux-x86_64-3.6/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99\n",
      "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/../common/maskApi.o build/temp.linux-x86_64-3.6/pycocotools/_mask.o -o build/lib.linux-x86_64-3.6/pycocotools/_mask.cpython-36m-x86_64-linux-gnu.so\n",
      "creating build/bdist.linux-x86_64\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/pycocotools\n",
      "copying build/lib.linux-x86_64-3.6/pycocotools/coco.py -> build/bdist.linux-x86_64/egg/pycocotools\n",
      "copying build/lib.linux-x86_64-3.6/pycocotools/__init__.py -> build/bdist.linux-x86_64/egg/pycocotools\n",
      "copying build/lib.linux-x86_64-3.6/pycocotools/mask.py -> build/bdist.linux-x86_64/egg/pycocotools\n",
      "copying build/lib.linux-x86_64-3.6/pycocotools/cocoeval.py -> build/bdist.linux-x86_64/egg/pycocotools\n",
      "copying build/lib.linux-x86_64-3.6/pycocotools/_mask.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg/pycocotools\n",
      "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/coco.py to coco.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/mask.py to mask.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/cocoeval.py to cocoeval.cpython-36.pyc\n",
      "creating stub loader for pycocotools/_mask.cpython-36m-x86_64-linux-gnu.so\n",
      "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/_mask.py to _mask.cpython-36.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
      "creating 'dist/pycocotools-2.0-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing pycocotools-2.0-py3.6-linux-x86_64.egg\n",
      "removing '/usr/local/lib/python3.6/dist-packages/pycocotools-2.0-py3.6-linux-x86_64.egg' (and everything under it)\n",
      "creating /usr/local/lib/python3.6/dist-packages/pycocotools-2.0-py3.6-linux-x86_64.egg\n",
      "Extracting pycocotools-2.0-py3.6-linux-x86_64.egg to /usr/local/lib/python3.6/dist-packages\n",
      "pycocotools 2.0 is already the active version in easy-install.pth\n",
      "\n",
      "Installed /usr/local/lib/python3.6/dist-packages/pycocotools-2.0-py3.6-linux-x86_64.egg\n",
      "Processing dependencies for pycocotools==2.0\n",
      "Searching for matplotlib==3.1.1\n",
      "Best match: matplotlib 3.1.1\n",
      "Adding matplotlib 3.1.1 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for Cython==0.29.14\n",
      "Best match: Cython 0.29.14\n",
      "Adding Cython 0.29.14 to easy-install.pth file\n",
      "Installing cygdb script to /usr/local/bin\n",
      "Installing cython script to /usr/local/bin\n",
      "Installing cythonize script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for setuptools==42.0.2\n",
      "Best match: setuptools 42.0.2\n",
      "Adding setuptools 42.0.2 to easy-install.pth file\n",
      "Installing easy_install script to /usr/local/bin\n",
      "Installing easy_install-3.8 script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for pyparsing==2.4.5\n",
      "Best match: pyparsing 2.4.5\n",
      "Adding pyparsing 2.4.5 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for cycler==0.10.0\n",
      "Best match: cycler 0.10.0\n",
      "Adding cycler 0.10.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for kiwisolver==1.1.0\n",
      "Best match: kiwisolver 1.1.0\n",
      "Adding kiwisolver 1.1.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for python-dateutil==2.8.1\n",
      "Best match: python-dateutil 2.8.1\n",
      "Adding python-dateutil 2.8.1 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for numpy==1.17.3\n",
      "Best match: numpy 1.17.3\n",
      "Adding numpy 1.17.3 to easy-install.pth file\n",
      "Installing f2py script to /usr/local/bin\n",
      "Installing f2py3 script to /usr/local/bin\n",
      "Installing f2py3.6 script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Searching for six==1.13.0\n",
      "Best match: six 1.13.0\n",
      "Adding six 1.13.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.6/dist-packages\n",
      "Finished processing dependencies for pycocotools==2.0\n",
      "/workspace/nvidia-examples/tensorrt/tftrt/examples/object_detection\n",
      "/home/vinhngx/vinh-all-data/TME_projects/tensorrt/tftrt/examples/image-classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "echo Download protobuf...\n",
      "mkdir -p $PROTOC_DIR\n",
      "pushd $PROTOC_DIR\n",
      "ARCH=$(uname -m)\n",
      "if [ \"$ARCH\" == \"aarch64\" ] ; then\n",
      "  filename=\"protoc-3.7.1-linux-aarch_64.zip\"\n",
      "elif [ \"$ARCH\" == \"x86_64\" ] ; then\n",
      "  filename=\"protoc-3.7.1-linux-x86_64.zip\"\n",
      "elif [ \"$ARCH\" == \"ppc64le\" ] ; then\n",
      "  filename=\"protoc-3.7.1-linux-ppcle_64.zip\"\n",
      "else\n",
      "  echo ERROR: $ARCH not supported.\n",
      "  exit 1;\n",
      "fi\n",
      "wget --no-check-certificate ${PROTO_BASE_URL}${filename}\n",
      "--2020-01-09 07:00:21--  https://github.com/google/protobuf/releases/download/v3.7.1/protoc-3.7.1-linux-x86_64.zip\n",
      "Resolving github.com (github.com)... 13.237.44.5\n",
      "Connecting to github.com (github.com)|13.237.44.5|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://github.com/protocolbuffers/protobuf/releases/download/v3.7.1/protoc-3.7.1-linux-x86_64.zip [following]\n",
      "--2020-01-09 07:00:22--  https://github.com/protocolbuffers/protobuf/releases/download/v3.7.1/protoc-3.7.1-linux-x86_64.zip\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/23357588/a97d5c80-50a2-11e9-869c-ffc2e5e27052?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200109%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200109T070022Z&X-Amz-Expires=300&X-Amz-Signature=bb5f7dfdea9113fec7936ab94acc3112334adeac224b74e1fdc7ec0e592b102a&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dprotoc-3.7.1-linux-x86_64.zip&response-content-type=application%2Foctet-stream [following]\n",
      "--2020-01-09 07:00:22--  https://github-production-release-asset-2e65be.s3.amazonaws.com/23357588/a97d5c80-50a2-11e9-869c-ffc2e5e27052?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200109%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200109T070022Z&X-Amz-Expires=300&X-Amz-Signature=bb5f7dfdea9113fec7936ab94acc3112334adeac224b74e1fdc7ec0e592b102a&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dprotoc-3.7.1-linux-x86_64.zip&response-content-type=application%2Foctet-stream\n",
      "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.217.42.244\n",
      "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.217.42.244|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1529306 (1.5M) [application/octet-stream]\n",
      "Saving to: ‘protoc-3.7.1-linux-x86_64.zip.4’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  3% 64.0K 23s\n",
      "    50K .......... .......... .......... .......... ..........  6% 64.6K 22s\n",
      "   100K .......... .......... .......... .......... .......... 10% 67.5K 21s\n",
      "   150K .......... .......... .......... .......... .......... 13% 72.6K 19s\n",
      "   200K .......... .......... .......... .......... .......... 16% 70.7K 18s\n",
      "   250K .......... .......... .......... .......... .......... 20% 97.0K 17s\n",
      "   300K .......... .......... .......... .......... .......... 23%  101K 15s\n",
      "   350K .......... .......... .......... .......... .......... 26%  103K 14s\n",
      "   400K .......... .......... .......... .......... .......... 30%  109K 13s\n",
      "   450K .......... .......... .......... .......... .......... 33%  186K 12s\n",
      "   500K .......... .......... .......... .......... .......... 36%  194K 11s\n",
      "   550K .......... .......... .......... .......... .......... 40%  205K 10s\n",
      "   600K .......... .......... .......... .......... .......... 43%  217K 9s\n",
      "   650K .......... .......... .......... .......... .......... 46%  218K 8s\n",
      "   700K .......... .......... .......... .......... .......... 50%  692K 7s\n",
      "   750K .......... .......... .......... .......... .......... 53%  241K 6s\n",
      "   800K .......... .......... .......... .......... .......... 56%  239K 6s\n",
      "   850K .......... .......... .......... .......... .......... 60% 1.24M 5s\n",
      "   900K .......... .......... .......... .......... .......... 63%  254K 4s\n",
      "   950K .......... .......... .......... .......... .......... 66%  940K 4s\n",
      "  1000K .......... .......... .......... .......... .......... 70%  257K 3s\n",
      "  1050K .......... .......... .......... .......... .......... 73% 1.04M 3s\n",
      "  1100K .......... .......... .......... .......... .......... 77%  267K 2s\n",
      "  1150K .......... .......... .......... .......... .......... 80% 1011K 2s\n",
      "  1200K .......... .......... .......... .......... .......... 83% 1.54M 2s\n",
      "  1250K .......... .......... .......... .......... .......... 87%  268K 1s\n",
      "  1300K .......... .......... .......... .......... .......... 90% 1.29M 1s\n",
      "  1350K .......... .......... .......... .......... .......... 93%  473K 1s\n",
      "  1400K .......... .......... .......... .......... .......... 97%  469K 0s\n",
      "  1450K .......... .......... .......... .......... ...       100% 1.53M=8.6s\n",
      "\n",
      "2020-01-09 07:00:32 (173 KB/s) - ‘protoc-3.7.1-linux-x86_64.zip.4’ saved [1529306/1529306]\n",
      "\n",
      "unzip -o ${filename}\n",
      "popd\n",
      "\n",
      "echo Compile object detection protobuf files...\n",
      "pushd $RESEARCH_DIR\n",
      "$PROTOC_DIR/bin/protoc object_detection/protos/*.proto --python_out=.\n",
      "popd\n",
      "\n",
      "echo Install tensorflow/models/research...\n",
      "pushd $RESEARCH_DIR\n",
      "pip install .\n",
      "popd\n",
      "\n",
      "echo Install tensorflow/models/research/slim...\n",
      "pushd $SLIM_DIR\n",
      "pip install .\n",
      "popd\n",
      "\n",
      "echo Install cocodataset/cocoapi/PythonAPI...\n",
      "pushd $PYCOCO_DIR\n",
      "python setup.py build_ext --inplace\n",
      "make\n",
      "# pip install .\n",
      "python setup.py install\n",
      "../common/maskApi.c: In function ‘rleDecode’:\n",
      "../common/maskApi.c:46:7: warning: this ‘for’ clause does not guard... [-Wmisleading-indentation]\n",
      "       for( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; v=!v; }}\n",
      "       ^~~\n",
      "../common/maskApi.c:46:49: note: ...this statement, but the latter is misleadingly indented as if it were guarded by the ‘for’\n",
      "       for( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; v=!v; }}\n",
      "                                                 ^\n",
      "../common/maskApi.c: In function ‘rleFrPoly’:\n",
      "../common/maskApi.c:166:3: warning: this ‘for’ clause does not guard... [-Wmisleading-indentation]\n",
      "   for(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];\n",
      "   ^~~\n",
      "../common/maskApi.c:166:54: note: ...this statement, but the latter is misleadingly indented as if it were guarded by the ‘for’\n",
      "   for(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];\n",
      "                                                      ^\n",
      "../common/maskApi.c:167:3: warning: this ‘for’ clause does not guard... [-Wmisleading-indentation]\n",
      "   for(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];\n",
      "   ^~~\n",
      "../common/maskApi.c:167:54: note: ...this statement, but the latter is misleadingly indented as if it were guarded by the ‘for’\n",
      "   for(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];\n",
      "                                                      ^\n",
      "../common/maskApi.c: In function ‘rleToString’:\n",
      "../common/maskApi.c:212:7: warning: this ‘if’ clause does not guard... [-Wmisleading-indentation]\n",
      "       if(more) c |= 0x20; c+=48; s[p++]=c;\n",
      "       ^~\n",
      "../common/maskApi.c:212:27: note: ...this statement, but the latter is misleadingly indented as if it were guarded by the ‘if’\n",
      "       if(more) c |= 0x20; c+=48; s[p++]=c;\n",
      "                           ^\n",
      "../common/maskApi.c: In function ‘rleFrString’:\n",
      "../common/maskApi.c:220:3: warning: this ‘while’ clause does not guard... [-Wmisleading-indentation]\n",
      "   while( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;\n",
      "   ^~~~~\n",
      "../common/maskApi.c:220:22: note: ...this statement, but the latter is misleadingly indented as if it were guarded by the ‘while’\n",
      "   while( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;\n",
      "                      ^~~~\n",
      "../common/maskApi.c:228:5: warning: this ‘if’ clause does not guard... [-Wmisleading-indentation]\n",
      "     if(m>2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;\n",
      "     ^~\n",
      "../common/maskApi.c:228:34: note: ...this statement, but the latter is misleadingly indented as if it were guarded by the ‘if’\n",
      "     if(m>2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;\n",
      "                                  ^~~~\n",
      "../common/maskApi.c: In function ‘rleToBbox’:\n",
      "../common/maskApi.c:141:31: warning: ‘xp’ may be used uninitialized in this function [-Wmaybe-uninitialized]\n",
      "       if(j%2==0) xp=x; else if(xp<x) { ys=0; ye=h-1; }\n",
      "                               ^\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "pycocotools.__pycache__._mask.cpython-36: module references __file__\n",
      "popd\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pushd /workspace/nvidia-examples/tensorrt/tftrt/examples/object_detection\n",
    "bash install_dependencies.sh;\n",
    "popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ii  libnvinfer-bin                         6.0.1-1+cuda10.2                  amd64        TensorRT binaries\r\n",
      "ii  libnvinfer-dev                         6.0.1-1+cuda10.2                  amd64        TensorRT development libraries and headers\r\n",
      "ii  libnvinfer-plugin-dev                  6.0.1-1+cuda10.2                  amd64        TensorRT plugin libraries\r\n",
      "ii  libnvinfer-plugin6                     6.0.1-1+cuda10.2                  amd64        TensorRT plugin libraries\r\n",
      "ii  libnvinfer6                            6.0.1-1+cuda10.2                  amd64        TensorRT runtime libraries\r\n",
      "ii  python3-libnvinfer                     6.0.1-1+cuda10.2                  amd64        Python 3 bindings for TensorRT\r\n",
      "ii  python3-libnvinfer-dev                 6.0.1-1+cuda10.2                  amd64        Python 3 development package for TensorRT\r\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "#check TensorRT version\n",
    "!dpkg -l | grep nvinfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "We first check that the correct Imagenet validation data folder has been mounted. In this experiment, we shall employ the Imagenet validation data to verify the accuracy and inference speed of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(data_dir, filename_pattern):\n",
    "    if data_dir == None:\n",
    "        return []\n",
    "    files = tf.gfile.Glob(os.path.join(data_dir, filename_pattern))\n",
    "    if files == []:\n",
    "        raise ValueError('Can not find any files in {} with '\n",
    "                         'pattern \"{}\"'.format(data_dir, filename_pattern))\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 128 validation files. \n",
      "/data/validation-00011-of-00128\n",
      "/data/validation-00086-of-00128\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_DATA_DIR = \"/data\"\n",
    "validation_files = get_files(VALIDATION_DATA_DIR, 'validation*')\n",
    "print('There are %d validation files. \\n%s\\n%s\\n...'%(len(validation_files), validation_files[0], validation_files[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF model checkpoint\n",
    "If not already downloaded, we will be downloading and working with a ResNet-50 v1 checkpoint from https://github.com/tensorflow/models/tree/master/research/slim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file '/saved_model/resnet_v1_50_2016_08_28.tar.gz' exists.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "FILE=/saved_model/resnet_v1_50_2016_08_28.tar.gz\n",
    "if [ -f $FILE ]; then\n",
    "   echo \"The file '$FILE' exists.\"\n",
    "else\n",
    "   echo \"The file '$FILE' in not found. Downloading...\"\n",
    "   wget -P /saved_model/ http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet_v1_50.ckpt\r\n"
     ]
    }
   ],
   "source": [
    "!tar -xzvf /saved_model/resnet_v1_50_2016_08_28.tar.gz -C /saved_model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "We define a few helper functions to read and preprocess Imagenet data from TFRecord files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some global variables\n",
    "BATCH_SIZE = 8\n",
    "SAVED_MODEL_DIR = \"/saved_model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_image_record(record):\n",
    "    feature_map = {\n",
    "        'image/encoded':          tf.FixedLenFeature([ ], tf.string, ''),\n",
    "        'image/class/label':      tf.FixedLenFeature([1], tf.int64,  -1),\n",
    "        'image/class/text':       tf.FixedLenFeature([ ], tf.string, ''),\n",
    "        'image/object/bbox/xmin': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/ymin': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/xmax': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/ymax': tf.VarLenFeature(dtype=tf.float32)\n",
    "    }\n",
    "    with tf.name_scope('deserialize_image_record'):\n",
    "        obj = tf.parse_single_example(record, feature_map)\n",
    "        imgdata = obj['image/encoded']\n",
    "        label   = tf.cast(obj['image/class/label'], tf.int32)\n",
    "        bbox    = tf.stack([obj['image/object/bbox/%s'%x].values\n",
    "                            for x in ['ymin', 'xmin', 'ymax', 'xmax']])\n",
    "        bbox = tf.transpose(tf.expand_dims(bbox, 0), [0,2,1])\n",
    "        text    = obj['image/class/text']\n",
    "        return imgdata, label, bbox, text\n",
    "\n",
    "from preprocessing import vgg_preprocessing\n",
    "def preprocess(record):\n",
    "    # Parse TFRecord\n",
    "    imgdata, label, bbox, text = deserialize_image_record(record)\n",
    "    label -= 1 # Change to 0-based (don't use background class)\n",
    "    try:    image = tf.image.decode_jpeg(imgdata, channels=3, fancy_upscaling=False, dct_method='INTEGER_FAST')\n",
    "    except: image = tf.image.decode_png(imgdata, channels=3)\n",
    "\n",
    "    image = vgg_preprocessing.preprocess_image(image, 224, 224, is_training=False)\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next are two functions to benchmark models speed and accuracy, either in a `graph_def` form or a `saved model` form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_frozen_graph(frozen_graph, SAVED_MODEL_DIR=None, BATCH_SIZE=8):\n",
    "    with tf.Session(graph=tf.Graph(), config=config) as sess:\n",
    "        # prepare dataset iterator\n",
    "        dataset = tf.data.TFRecordDataset(validation_files)    \n",
    "        dataset = dataset.apply(tf.contrib.data.map_and_batch(map_func=preprocess, batch_size=BATCH_SIZE, num_parallel_calls=20))\n",
    "\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        output_node = tf.import_graph_def(\n",
    "            frozen_graph,\n",
    "            return_elements=['classes'],\n",
    "            name=\"\")\n",
    "        \n",
    "        print('Warming up for 50 batches...')\n",
    "        for _ in range (50):\n",
    "            sess.run(['classes:0'], feed_dict={\"input:0\": sess.run(next_element)[0]})\n",
    "\n",
    "        num_hits = 0\n",
    "        num_predict = 0\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            while True:        \n",
    "                image_data = sess.run(next_element)    \n",
    "                img = image_data[0]\n",
    "                label = image_data[1].squeeze()\n",
    "                output = sess.run(['classes:0'], feed_dict={\"input:0\": img})\n",
    "                prediction = output[0]\n",
    "                num_hits += np.sum(prediction == label)\n",
    "                num_predict += len(prediction)\n",
    "        except tf.errors.OutOfRangeError as e:\n",
    "            pass\n",
    "\n",
    "        print('Accuracy: %.2f%%'%(100*num_hits/num_predict)) \n",
    "        print('Inference speed: %.2f samples/s'%(num_predict/(time.time()-start_time)))\n",
    "        \n",
    "        #Optionally, save model for serving if an ouput directory argument is presented\n",
    "        if SAVED_MODEL_DIR:\n",
    "            print('Saving model to %s'%SAVED_MODEL_DIR)\n",
    "            tf.saved_model.simple_save(\n",
    "                session=sess,\n",
    "                export_dir=SAVED_MODEL_DIR,\n",
    "                inputs={\"input\":tf.get_default_graph().get_tensor_by_name(\"input:0\")},\n",
    "                outputs={\"classes\":tf.get_default_graph().get_tensor_by_name(\"classes:0\")},\n",
    "                legacy_init_op=None\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_saved_model(SAVED_MODEL_DIR, BATCH_SIZE=8):\n",
    "    with tf.Session(graph=tf.Graph(), config=config) as sess:\n",
    "        # prepare dataset iterator\n",
    "        dataset = tf.data.TFRecordDataset(validation_files)    \n",
    "        dataset = dataset.apply(tf.contrib.data.map_and_batch(map_func=preprocess, batch_size=BATCH_SIZE, num_parallel_calls=20))\n",
    "\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        tf.saved_model.loader.load(\n",
    "            sess, [tf.saved_model.tag_constants.SERVING], SAVED_MODEL_DIR)\n",
    "\n",
    "        print('Warming up for 50 batches...')\n",
    "        for _ in range (50):\n",
    "            sess.run(['classes:0'], feed_dict={\"input:0\": sess.run(next_element)[0]})\n",
    "\n",
    "        print('Benchmarking inference engine...')\n",
    "        num_hits = 0\n",
    "        num_predict = 0\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            while True:        \n",
    "                image_data = sess.run(next_element)    \n",
    "                img = image_data[0]\n",
    "                label = image_data[1].squeeze()\n",
    "                output = sess.run(['classes:0'], feed_dict={\"input:0\": img})            \n",
    "                prediction = output[0]\n",
    "                num_hits += np.sum(prediction == label)\n",
    "                num_predict += len(prediction)\n",
    "        except tf.errors.OutOfRangeError as e:\n",
    "            pass\n",
    "\n",
    "        print('Accuracy: %.2f%%'%(100*num_hits/num_predict))\n",
    "        print('Inference speed: %.2f samples/s'%(num_predict/(time.time()-start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Verifying the orignal FP32 model\n",
    "\n",
    "We first load and benchmark the Resnet-v1-50 model from TF slim. Note that the checkpoint downloaded from http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz doesn't come with any meta data, therefore we will need to employ TF Slim Net factory to get the model definition. The newer checkpoints generated by Tensorflow generally comes with enough meta information to load the network from the achirve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nets.nets_factory\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.Session(config=config) as sess:\n",
    "        tf_input = tf.placeholder(tf.float32, [None, 224, 224, 3], name='input')\n",
    "        network_fn = nets.nets_factory.get_network_fn('resnet_v1_50', 1000,\n",
    "                                                      is_training=False)\n",
    "        tf_net, tf_end_points = network_fn(tf_input)\n",
    "                \n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, SAVED_MODEL_DIR+\"resnet_v1_50.ckpt\")\n",
    "        \n",
    "        tf_output = tf.identity(tf_net, name='logits')\n",
    "        tf_output_classes = tf.argmax(tf_output, axis=1, name='classes')        \n",
    "        #tf_output_classes = tf.reshape(tf_output_classes, (BATCH_SIZE,), name='classes')\n",
    "        \n",
    "        # freeze graph\n",
    "        fp32_frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            sess.graph_def,\n",
    "            output_node_names=['logits', 'classes']\n",
    "        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 50 batches...\n",
      "Accuracy: 75.18%\n",
      "Inference speed: 668.93 samples/s\n",
      "Saving model to /saved_model/model/Resnet_FP32/1\n"
     ]
    }
   ],
   "source": [
    "FP32_SAVED_MODEL_DIR = SAVED_MODEL_DIR+\"model/Resnet_FP32/1\"\n",
    "!rm -rf $FP32_SAVED_MODEL_DIR\n",
    "\n",
    "benchmark_frozen_graph(fp32_frozen_graph, FP32_SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-09 07:04:03.274276: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 224, 224, 3)\n",
      "        name: input:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['classes'] tensor_info:\n",
      "        dtype: DT_INT64\n",
      "        shape: (-1)\n",
      "        name: classes:0\n",
      "  Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --all --dir $FP32_SAVED_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 50 batches...\n",
      "Benchmarking inference engine...\n",
      "Accuracy: 75.18%\n",
      "Inference speed: 739.00 samples/s\n"
     ]
    }
   ],
   "source": [
    "benchmark_saved_model(FP32_SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. Creating TF-TRT FP32 model\n",
    "\n",
    "Next, we convert the naitive TF FP32 model to TF-TRT FP32, then verify model accuracy and inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Alternatively, old API:\\ntrt_fp32_graph = trt.create_inference_graph(\\n    input_graph_def=fp32_frozen_graph,\\n    outputs=[\\'classes\\'],\\n    max_batch_size=BATCH_SIZE,\\n    precision_mode=\"FP32\")\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we create the TFTRT FP32 engine\n",
    "converter = trt.TrtGraphConverter(input_graph_def=fp32_frozen_graph,\n",
    "                                  max_batch_size=BATCH_SIZE,\n",
    "                                  precision_mode=trt.TrtPrecisionMode.FP32,\n",
    "                                  nodes_blacklist=['classes', 'logits'])\n",
    "trt_fp32_graph = converter.convert()\n",
    "\n",
    "\"\"\"\n",
    "# Alternatively, old API:\n",
    "trt_fp32_graph = trt.create_inference_graph(\n",
    "    input_graph_def=fp32_frozen_graph,\n",
    "    outputs=['classes'],\n",
    "    max_batch_size=BATCH_SIZE,\n",
    "    precision_mode=\"FP32\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 50 batches...\n",
      "Accuracy: 75.18%\n",
      "Inference speed: 968.44 samples/s\n",
      "Saving model to /saved_model/model/Resnet_TRT_FP32/1\n"
     ]
    }
   ],
   "source": [
    "TRT_FP32_SAVED_MODEL_DIR = SAVED_MODEL_DIR+\"model/Resnet_TRT_FP32/1\"\n",
    "!rm -rf $TRT_FP32_SAVED_MODEL_DIR\n",
    "\n",
    "benchmark_frozen_graph(trt_fp32_graph, TRT_FP32_SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-09 07:11:09.291882: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 224, 224, 3)\n",
      "        name: input:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['classes'] tensor_info:\n",
      "        dtype: DT_INT64\n",
      "        shape: unknown_rank\n",
      "        name: classes:0\n",
      "  Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --all --dir $TRT_FP32_SAVED_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 50 batches...\n",
      "Benchmarking inference engine...\n",
      "Accuracy: 75.18%\n",
      "Inference speed: 618.83 samples/s\n"
     ]
    }
   ],
   "source": [
    "benchmark_saved_model(TRT_FP32_SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## 4. Creating TF-TRT FP16 model\n",
    "\n",
    "\n",
    "Next, we convert the naitive TF FP32 model to TF-TRT FP16, then verify model accuracy and inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Alternatively, old API:\\ntrt_fp16_graph = trt.create_inference_graph(\\n    input_graph_def=fp32_frozen_graph,\\n    outputs=[\\'classes\\'],\\n    max_batch_size=BATCH_SIZE,\\n    precision_mode=\"FP16\")\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we create the TFTRT FP16 engine\n",
    "converter = trt.TrtGraphConverter(input_graph_def=fp32_frozen_graph,\n",
    "                                  max_batch_size=BATCH_SIZE,\n",
    "                                  precision_mode=trt.TrtPrecisionMode.FP16,\n",
    "                                  nodes_blacklist=['classes', 'logits'])\n",
    "trt_fp16_graph = converter.convert()\n",
    "\n",
    "\"\"\"\n",
    "# Alternatively, old API:\n",
    "trt_fp16_graph = trt.create_inference_graph(\n",
    "    input_graph_def=fp32_frozen_graph,\n",
    "    outputs=['classes'],\n",
    "    max_batch_size=BATCH_SIZE,\n",
    "    precision_mode=\"FP16\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 50 batches...\n",
      "Accuracy: 75.18%\n",
      "Inference speed: 726.11 samples/s\n",
      "Saving model to /saved_model//model/Resnet_TRT_FP16/1\n"
     ]
    }
   ],
   "source": [
    "TRT_FP16_SAVED_MODEL_DIR = SAVED_MODEL_DIR+\"/model/Resnet_TRT_FP16/1\"\n",
    "!rm -rf $TRT_FP16_SAVED_MODEL_DIR\n",
    "\n",
    "benchmark_frozen_graph(trt_fp16_graph, TRT_FP16_SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 50 batches...\n",
      "Benchmarking inference engine...\n",
      "Accuracy: 75.18%\n",
      "Inference speed: 782.17 samples/s\n"
     ]
    }
   ],
   "source": [
    "benchmark_saved_model(TRT_FP16_SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. Creating TF-TRT INT8 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating TF-TRT INT8 inference model requires two steps:\n",
    "\n",
    "- Step 1: Prepare a calibration data\n",
    "\n",
    "- Step 2: Convert and calibrate the TF-TRT INT8 inference engine\n",
    "\n",
    "### Step 1: Prepare a calibration dataset\n",
    "\n",
    "Creating TF-TRT INT8 model requires a small calibration dataset. This data set ideally should represent the test data in production well, and will be used to create a value histogram for each layer in the neural network for effective 8-bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration data shape:  (16, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "num_calibration_batches = 2\n",
    "batched_input = np.zeros((BATCH_SIZE * num_calibration_batches, 224, 224, 3), dtype=np.float32)\n",
    "\n",
    "with tf.Session(graph=tf.Graph(), config=config) as sess:\n",
    "    # prepare dataset iterator\n",
    "    dataset = tf.data.TFRecordDataset(validation_files)    \n",
    "    dataset = dataset.apply(tf.contrib.data.map_and_batch(map_func=preprocess, batch_size=BATCH_SIZE, num_parallel_calls=20))\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_element = iterator.get_next()\n",
    "    for i in range(num_calibration_batches):\n",
    "        batched_input[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :] = sess.run(next_element)[0]\n",
    "\n",
    "#batched_input = tf.constant(batched_input)\n",
    "print('Calibration data shape: ', batched_input.shape)\n",
    "\n",
    "def calibration_input_fn_gen():\n",
    "    for i in range(num_calibration_batches):\n",
    "        yield batched_input[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :]\n",
    "        \n",
    "calibration_input_fn = calibration_input_fn_gen()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2: Convert and calibrate the TF-TRT INT8 inference engine\n",
    "\n",
    "The calibration step may take a while to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a directory to write the saved model\n",
    "INT8_SAVED_MODEL_DIR =  SAVED_MODEL_DIR + \"_TFTRT_INT8/1\"\n",
    "!rm -rf $INT8_SAVED_MODEL_DIR\n",
    "\n",
    "#Now we create the TFTRT FP16 engine\n",
    "converter = trt.TrtGraphConverter(input_graph_def=fp32_frozen_graph,\n",
    "                                  max_batch_size=BATCH_SIZE,\n",
    "                                  precision_mode=trt.TrtPrecisionMode.INT8,\n",
    "                                  nodes_blacklist=['classes', 'logits'])\n",
    "trt_int8_graph = converter.convert()\n",
    "\n",
    "\n",
    "# Run calibration for num_calibration_batches times.\n",
    "trt_int8_calibrated_graph = converter.calibrate(\n",
    "      fetch_names=['classes:0'],\n",
    "      num_runs=num_calibration_batches,\n",
    "      feed_dict_fn=lambda: {\"input:0\": next(calibration_input_fn)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 50 batches...\n",
      "Accuracy: 75.16%\n",
      "Inference speed: 1336.62 samples/s\n",
      "Saving model to /saved_model//model/Resnet_TRT_INT8/1\n"
     ]
    }
   ],
   "source": [
    "INT8_SAVED_MODEL_DIR = SAVED_MODEL_DIR + '/model/Resnet_TRT_INT8/1'\n",
    "!rm -rf $INT8_SAVED_MODEL_DIR\n",
    "\n",
    "benchmark_frozen_graph(trt_int8_calibrated_graph, INT8_SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-09 07:22:09.282118: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 224, 224, 3)\n",
      "        name: input:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['classes'] tensor_info:\n",
      "        dtype: DT_INT64\n",
      "        shape: unknown_rank\n",
      "        name: classes:0\n",
      "  Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --all --dir $INT8_SAVED_MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we reload and verify the performance of the INT8 saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 50 batches...\n",
      "Benchmarking inference engine...\n",
      "Accuracy: 75.15%\n",
      "Inference speed: 1641.23 samples/s\n"
     ]
    }
   ],
   "source": [
    "benchmark_saved_model(INT8_SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6. Calibrating TF-TRT INT8 model with raw JPEG images\n",
    "\n",
    "As an alternative to taking data in TFRecords format, in this section, we demonstrate the process of calibrating TFTRT INT-8 model from a directory of raw JPEG images. We asume that raw images have been mounted to the directory `/data/Calibration_data`.\n",
    "\n",
    "As a rule of thumb, calibration data should be a small but representative set of images that is similar to what is expected in deployment. Empirically, for common network architectures trained on imagenet data, calibration data of size 500-1000 provide good accuracy. As such, a good strategy for a dataset such as imagenet is to choose one sample from each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 45 calibration files. \n",
      "/data/Calibration_data/ILSVRC2012_val_00028553.JPEG\n",
      "/data/Calibration_data/ILSVRC2012_val_00030154.JPEG\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "data_directory = \"/data/Calibration_data\"\n",
    "calibration_files = [os.path.join(path, name) for path, _, files in os.walk(data_directory) for name in files]\n",
    "print('There are %d calibration files. \\n%s\\n%s\\n...'%(len(calibration_files), calibration_files[0], calibration_files[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(filepath):\n",
    "    image = tf.read_file(filepath)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = vgg_preprocessing.preprocess_image(image, 224, 224, is_training=False)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration data shape:  (16, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "num_calibration_batches = 2\n",
    "batched_input = np.zeros((BATCH_SIZE * num_calibration_batches, 224, 224, 3), dtype=np.float32)\n",
    "\n",
    "with tf.Session(graph=tf.Graph(), config=config) as sess:\n",
    "    # prepare dataset iterator\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(calibration_files)\n",
    "    dataset = dataset.apply(tf.contrib.data.map_and_batch(map_func=parse_file, batch_size=BATCH_SIZE, num_parallel_calls=20))\n",
    "    dataset = dataset.repeat(None)\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_element = iterator.get_next()\n",
    "    for i in range(num_calibration_batches):\n",
    "        batched_input[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :] = sess.run(next_element)[0]\n",
    "\n",
    "#batched_input = tf.constant(batched_input)\n",
    "print('Calibration data shape: ', batched_input.shape)\n",
    "\n",
    "def calibration_input_fn_gen():\n",
    "    for i in range(num_calibration_batches):\n",
    "        yield batched_input[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :]\n",
    "        \n",
    "calibration_input_fn = calibration_input_fn_gen()       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we proceed with the two-stage process of creating and calibrating TFTRT INT8 model.\n",
    "\n",
    "### Convert and calibrate the TF-TRT INT8 inference engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a directory to write the saved model\n",
    "INT8_SAVED_MODEL_DIR =  SAVED_MODEL_DIR + \"_TFTRT_INT8/1\"\n",
    "!rm -rf $INT8_SAVED_MODEL_DIR\n",
    "\n",
    "#Now we create the TFTRT FP16 engine\n",
    "converter = trt.TrtGraphConverter(input_graph_def=fp32_frozen_graph,\n",
    "                                  max_batch_size=BATCH_SIZE,\n",
    "                                  precision_mode=trt.TrtPrecisionMode.INT8,\n",
    "                                  nodes_blacklist=['classes', 'logits'])\n",
    "trt_int8_graph = converter.convert()\n",
    "\n",
    "\n",
    "# Run calibration for num_calibration_batches times.\n",
    "trt_int8_calibrated_graph = converter.calibrate(\n",
    "      fetch_names=['classes:0'],\n",
    "      num_runs=num_calibration_batches,\n",
    "      feed_dict_fn=lambda: {\"input:0\": next(calibration_input_fn)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can benchmark the speed and accuracy of the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_frozen_graph(trt_int8_calibrated_graph, INT8_SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we have demonstrated the process of creating TF-TRT inference models from an original TF FP32 checkpoint. In every case, we have also verified the accuracy and speed to the resulting models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
