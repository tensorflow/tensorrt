{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54127018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c86e850",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/tensorrt_tftrt_dynamic_shapes/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Using Dynamic Shapes with TensorFlow TensorRT\n",
    "\n",
    "The NVIDIA TensorRT is a library that facilitates high performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network, which consists of a network definition and a set of trained parameters, and produces a highly optimized runtime engine which performs inference for that network. \n",
    "\n",
    "TensorFlow™ integration with TensorRT™ (TF-TRT) optimizes and executes compatible subgraphs, allowing TensorFlow to execute the remaining graph. While you can still use TensorFlow's wide and flexible feature set, TensorRT will parse the model and apply optimizations to the portions of the graph wherever possible.\n",
    "\n",
    "In this notebook demonstrates the use of dynamic shape tensors when using TensorFlow-TensorRT\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "If you are unfamiliar with how TensorFlow TensorRT works, you can refer to this [video](https://www.youtube.com/watch?v=w7871kMiAs8) for a quick overview. Some understanding of how TF-TRT works is required to digest the information in the following section. A quick and dirty explaination of the above is as follows: TF-TRT partitions the network graph into supported and unsupported sub-graphs. For each of these supported subgraphs, TRTEngineOp builds a TensorRT Engine. With this information in mind, let's proceed to the task at hand.\n",
    "\n",
    "TensorFlow TensorRT has two concepts relevent to this discussion:\n",
    "* Dynamic Ops\n",
    "* Dynamic Shape\n",
    "\n",
    "#### Explaining Dynamic Ops\n",
    "\n",
    "Dynamic Ops can be treated as a mode which let's users leverage the optimized model \"implicit shape\" mode, ie, if the model's input tensor shape is defined as(example) `[?, ?, ?, 3]`. How does this work? The TRTEngineOp creates the TensorRT engine at inference time with the shape of the input tensor (Let's say, `[8, 224, 224, 3]`). So up on execution, if we supply a tensor with a shape (say `[16, 224, 224, 3]`) another engine will be created. While this provides flexibility, the downside is that each TRT Engine consumes memory (a set of model weights for each \"profile\").\n",
    "\n",
    "#### Explaining Dynamic Shapes\n",
    "\n",
    "Dynamic Shape mode reqires the user to define, `minimum`, `optimial` and `maximum` shapes for the input tensor. This shifts the task at hand from being one about supporting implict tensor shape to supporting a set of explict batch shapes. The engine built in this case can handle any shape between the `minimum` and `maximum` shape, without a need for building separate engines.\n",
    "\n",
    "For a visual representation of the above, refer to the image below. The image on the right shows the scenerio where the use of three different shapes has resulted in three different engines as opposed to the one for dynamic shapes.\n",
    "\n",
    "![Dynamic Ops vs Dynamic Shapes](img/1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6359caa7",
   "metadata": {},
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7ad5b",
   "metadata": {},
   "source": [
    "TensorFlow-TensorRT comes packaged with TensorFlow, so if you have TensorFlow setup, and are running this on any NVIDIA GPU with CUDA cores (preferablly a Volta, Turing, Ampere or newer generation GPU with Tensor cores) you can proceed. You can also choose to run this inside our [TensorFlow container](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorflow) which comes packaged with a host of software which can help acclerate your workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd72a995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d640ccb4",
   "metadata": {},
   "source": [
    "#### Model\n",
    "For this demonstration, a simple ResNet-50 Model trained on the imagenet dataset is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d737cc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 19:22:02.968545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-12 19:22:02.995428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-12 19:22:02.995597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-12 19:22:02.996066: I tensorflow/core/platform/cpu_feature_guard.cc:152] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-12 19:22:03.030744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-12 19:22:03.030892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-12 19:22:03.031015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-12 19:22:03.309980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-12 19:22:03.310131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-12 19:22:03.310243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-12 19:22:03.310344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7863 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:09:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 19:22:07.287423: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: resnet50_saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50(weights='imagenet')\n",
    "model.save('resnet50_saved_model') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cd9a0f",
   "metadata": {},
   "source": [
    "Let's take a look at the shape of the input and output tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dabe11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input_1'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 224, 224, 3)\n",
      "        name: serving_default_input_1:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['predictions'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1000)\n",
      "        name: StatefulPartitionedCall:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "\n",
      "Concrete Functions:\n",
      "  Function Name: '__call__'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_1: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_1: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "\n",
      "  Function Name: '_default_save_signature'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_1: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')\n",
      "\n",
      "  Function Name: 'call_and_return_all_conditional_losses'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_1: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_1: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --all --dir resnet50_saved_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9853e1f",
   "metadata": {},
   "source": [
    "### TF-TRT using dynamic shape\n",
    "\n",
    "Dynamic shape mode requires TRT optimization profiles: “A TensorRT optimization profile describes the possible min/max values of each dynamic input shape along with an optimum value. These values are used by the TensorRT builder to select the best kernel for the optimum value among those kernels that are valid for all input tensors in the [min, max] range.”\n",
    "\n",
    "In dynamic shape mode, optimization profiles are needed to be defined before a TensorRT engine can be created. Therefore users are required to provide input tensors using the input_fn when converter.build is called, so that TF-TRT can automatically generate the needed optimization profiles (guided by the shapes of these input tensors) to build a single engine that can take different input shapes.\n",
    "\n",
    "The function below requires a `minimum`, `optimal`, and `maximum` batch shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dc7c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn():\n",
    "  input_shapes = [[(16, 224, 224, 3)],\n",
    "                  [(32, 224, 224, 3)],\n",
    "                  [(64, 224, 224, 3)]]\n",
    "  for shapes in input_shapes:\n",
    "    yield [np.zeros(x, dtype=np.float32) for x in shapes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bb8713b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to TF-TRT FP32...\n",
      "INFO:tensorflow:Linked TensorRT version: (8, 2, 4)\n",
      "INFO:tensorflow:Loaded TensorRT version: (8, 2, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 19:22:23.416416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-12 19:22:23.416556: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2022-05-12 19:22:23.416638: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2022-05-12 19:22:23.416946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-12 19:22:23.417066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-12 19:22:23.417166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-12 19:22:23.425144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-12 19:22:23.425287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-12 19:22:23.425375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7863 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:09:00.0, compute capability: 8.6\n",
      "2022-05-12 19:22:23.477929: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1191] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: Graph size after: 1202 nodes (878), 1857 edges (1533), time = 16.619ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.007ms.\n",
      "\n",
      "2022-05-12 19:22:24.745551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-12 19:22:24.745701: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2022-05-12 19:22:24.745769: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2022-05-12 19:22:24.746281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-12 19:22:24.746398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-12 19:22:24.746500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-12 19:22:24.746644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-12 19:22:24.746759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-12 19:22:24.746843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7863 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:09:00.0, compute capability: 8.6\n",
      "2022-05-12 19:22:24.938672: W tensorflow/compiler/tf2tensorrt/convert/trt_optimization_pass.cc:192] Calibration with FP32 or FP16 is not implemented. Falling back to use_calibration = False.Note that the default value of use_calibration is True.\n",
      "2022-05-12 19:22:25.059325: W tensorflow/compiler/tf2tensorrt/segment/segment.cc:954] \n",
      "\n",
      "################################################################################\n",
      "TensorRT unsupported/non-converted OP Report:\n",
      "\t- NoOp -> 2x\n",
      "\t- Identity -> 1x\n",
      "\t- Placeholder -> 1x\n",
      "--------------------------------------------------------------------------------\n",
      "\t- Total nonconverted OPs: 4\n",
      "\t- Total nonconverted OP Types: 3\n",
      "For more information see https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops.\n",
      "################################################################################\n",
      "\n",
      "2022-05-12 19:22:25.074575: W tensorflow/compiler/tf2tensorrt/segment/segment.cc:1282] The environment variable TF_TRT_MAX_ALLOWED_ENGINES=20 has no effect since there are only 1 TRT Engines with  at least minimum_segment_size=3 nodes.\n",
      "2022-05-12 19:22:25.076564: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:796] Number of TensorRT candidate segments: 1\n",
      "2022-05-12 19:22:25.080998: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:913] Replaced segment 0 consisting of 500 nodes by TRTEngineOp_000_000.\n",
      "2022-05-12 19:22:25.168205: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1191] Optimization results for grappler item: tf_graph\n",
      "  model_pruner: Graph size after: 880 nodes (-322), 1535 edges (-322), time = 4.654ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.052ms.\n",
      "  layout: Graph size after: 884 nodes (4), 1539 edges (4), time = 23.704ms.\n",
      "  dependency_optimizer: Graph size after: 560 nodes (-324), 575 edges (-964), time = 8.073ms.\n",
      "  constant_folding: Graph size after: 558 nodes (-2), 573 edges (-2), time = 12.895ms.\n",
      "  common_subgraph_elimination: Graph size after: 508 nodes (-50), 573 edges (0), time = 38.207ms.\n",
      "  TensorRTOptimizer: Graph size after: 9 nodes (-499), 2 edges (-571), time = 149.036ms.\n",
      "  constant_folding: Graph size after: 3 nodes (-6), 2 edges (0), time = 0.709ms.\n",
      "Optimization results for grappler item: TRTEngineOp_000_000_native_segment\n",
      "  model_pruner: Graph size after: 450 nodes (-52), 469 edges (-52), time = 2.329ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.024ms.\n",
      "  layout: Graph size after: 450 nodes (0), 469 edges (0), time = 7.144ms.\n",
      "  dependency_optimizer: Graph size after: 450 nodes (0), 469 edges (0), time = 2.589ms.\n",
      "  constant_folding: Graph size after: 450 nodes (0), 469 edges (0), time = 7.344ms.\n",
      "  common_subgraph_elimination: Graph size after: 450 nodes (0), 469 edges (0), time = 38.239ms.\n",
      "  TensorRTOptimizer: Graph size after: 450 nodes (0), 469 edges (0), time = 0.583ms.\n",
      "  constant_folding: Graph size after: 450 nodes (0), 469 edges (0), time = 7.77ms.\n",
      "\n",
      "2022-05-12 19:22:26.009110: I tensorflow/compiler/tf2tensorrt/common/utils.cc:100] Linked TensorRT version: 8.2.4\n",
      "2022-05-12 19:22:26.009219: I tensorflow/compiler/tf2tensorrt/common/utils.cc:102] Loaded TensorRT version: 8.2.4\n",
      "2022-05-12 19:22:26.759886: I tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc:1255] [TF-TRT] Sparse compute capability is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: resnet50_saved_model_TFTRT_FP32/assets\n",
      "Done Converting to TF-TRT FP32\n"
     ]
    }
   ],
   "source": [
    "print('Converting to TF-TRT FP32...')\n",
    "\n",
    "converter = trt.TrtGraphConverterV2(input_saved_model_dir='resnet50_saved_model',\n",
    "                                   precision_mode=trt.TrtPrecisionMode.FP32,\n",
    "                                    max_workspace_size_bytes=1<<32)\n",
    "converter.convert()\n",
    "converter.build(input_fn)\n",
    "converter.save(output_saved_model_dir='resnet50_saved_model_TFTRT_FP32')\n",
    "print('Done Converting to TF-TRT FP32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bcfd336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input_1'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 224, 224, 3)\n",
      "        name: serving_default_input_1:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['predictions'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1000)\n",
      "        name: PartitionedCall:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "2022-05-12 19:23:09.362483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-12 19:23:09.391170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-12 19:23:09.391335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "\n",
      "Concrete Functions:\n",
      "  Function Name: '__call__'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_1: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_1: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "\n",
      "  Function Name: '_default_save_signature'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_1: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')\n",
      "\n",
      "  Function Name: 'call_and_return_all_conditional_losses'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_1: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_1: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --all --dir resnet50_saved_model_TFTRT_FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3cba80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use empty tensors as dummy input to make a quick and dirty benchmarking util.\n",
    "# Feel free to add your own dataloader if you so choose to \n",
    "\n",
    "def benchmark_tftrt(input_saved_model, batch_size):\n",
    "    batched_input = tf.random.uniform(shape=[batch_size, 224, 224, 3])\n",
    "    \n",
    "    saved_model_loaded = tf.saved_model.load(input_saved_model, tags=[tag_constants.SERVING])\n",
    "    infer = saved_model_loaded.signatures['serving_default']\n",
    "\n",
    "    N_warmup_run = 50\n",
    "    N_run = 1000\n",
    "    elapsed_time = []\n",
    "\n",
    "    for i in range(N_warmup_run):\n",
    "      labeling = infer(batched_input)\n",
    "\n",
    "    for i in range(N_run):\n",
    "      start_time = time.time()\n",
    "      labeling = infer(batched_input)\n",
    "      end_time = time.time()\n",
    "      elapsed_time = np.append(elapsed_time, end_time - start_time)\n",
    "      if i % 50 == 0:\n",
    "        print('Step {}: {:4.1f}ms'.format(i, (elapsed_time[-50:].mean()) * 1000))\n",
    "\n",
    "    print('Throughput: {:.0f} images/s'.format(N_run * batch_size / elapsed_time.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c41d258f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 19:23:14.908227: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:36] TF-TRT Warning: DefaultLogger Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0:  7.6ms\n",
      "Step 50:  7.5ms\n",
      "Step 100:  7.5ms\n",
      "Step 150:  7.6ms\n",
      "Step 200:  7.6ms\n",
      "Step 250:  7.6ms\n",
      "Step 300:  7.6ms\n",
      "Step 350:  7.6ms\n",
      "Step 400:  7.6ms\n",
      "Step 450:  7.6ms\n",
      "Step 500:  7.6ms\n",
      "Step 550:  7.6ms\n",
      "Step 600:  7.6ms\n",
      "Step 650:  7.6ms\n",
      "Step 700:  7.6ms\n",
      "Step 750:  7.6ms\n",
      "Step 800:  7.6ms\n",
      "Step 850:  7.6ms\n",
      "Step 900:  7.6ms\n",
      "Step 950:  7.6ms\n",
      "Throughput: 2108 images/s\n"
     ]
    }
   ],
   "source": [
    "benchmark_tftrt('resnet50_saved_model_TFTRT_FP32', 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e4854b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 19:23:26.768667: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:36] TF-TRT Warning: DefaultLogger Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: 13.9ms\n",
      "Step 50: 13.9ms\n",
      "Step 100: 13.8ms\n",
      "Step 150: 13.9ms\n",
      "Step 200: 13.9ms\n",
      "Step 250: 13.9ms\n",
      "Step 300: 13.9ms\n",
      "Step 350: 13.9ms\n",
      "Step 400: 13.9ms\n",
      "Step 450: 13.9ms\n",
      "Step 500: 13.9ms\n",
      "Step 550: 13.9ms\n",
      "Step 600: 13.9ms\n",
      "Step 650: 13.9ms\n",
      "Step 700: 13.9ms\n",
      "Step 750: 13.9ms\n",
      "Step 800: 13.9ms\n",
      "Step 850: 13.9ms\n",
      "Step 900: 13.9ms\n",
      "Step 950: 13.9ms\n",
      "Throughput: 2303 images/s\n"
     ]
    }
   ],
   "source": [
    "benchmark_tftrt('resnet50_saved_model_TFTRT_FP32', 32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
